% Template for the submission to:
%   The Annals of Probability           [aop]
%   The Annals of Applied Probability   [aap]
%   The Annals of Statistics            [aos]
%   The Annals of Applied Statistics    [aoas]
%   Stochastic Systems                  [ssy]
%
%Author: In this template, the places where you need to add information
%        (or delete line) are indicated by {???}.  Mostly the information
%        required is obvious, but some explanations are given in lines starting
%Author:
%All other lines should be ignored.  After editing, there should be
%no instances of ??? after this line.

% use option [preprint] to remove info line at bottom
% journal options: aop,aap,aos,aoas,ssy
% natbib option: authoryear
\documentclass[aoas,preprint,authoryear]{imsart}

\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{natbib}
\RequirePackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}

% provide arXiv number if available:
%\arxiv{arXiv:0000.0000}

% put your definitions there:
\startlocaldefs
\endlocaldefs

\begin{document}

\begin{frontmatter}
\title{Learning to Transfer with\\
       Triply Adversarial Neural Networks}

% indicate corresponding author with \corref{}
% \author{\fnms{John} \snm{Smith}\corref{}\ead[label=e1]{smith@foo.com}\thanksref{t1}}
% \thankstext{t1}{Thanks to somebody}
% \address{line 1\\ line 2\\ printead{e1}}
% \affiliation{Some University}

\author{\fnms{Gilles} \snm{Louppe}}
\affiliation{New York University}

\begin{abstract}

Learning under covariate shift, also known as transfer learning or domain
adaptation, arises in supervised learning whenever test instances are governed
by a distribution that may be arbitrarily different from the distribution of the
training instances. This problem has traditionally been solved either by
reweighting training instances or by learning feature representations that are
robust to domain adaptation. In this work, we propose a new paradigm which
consists instead in learning to transform training instances into test
instances. We extend the generative adversarial networks framework
\citep{goodfellow2014generative} to a triply adversarial process: a transformer
network $T$ for generating test instances from training instances, a
discriminative network $D$ for estimating whether an instance comes either from
the training or the test distributions, and a classifier network $C$ for
classifying training instances in the projected space. Accordingly, this
3-player game results in a network $T$ capable of transforming training
instances into test instances, while preserving the separation between classes.
Experimental results demonstrate the potential of this novative approach and
illustrate the importance of both regularization and network architecture for
controlling the learned transformation.

\end{abstract}

\end{frontmatter}

\section{Introduction}

\section{Method}

\section{Experiments}

\section{Related work}

\section{Conclusions}

% Outline:
% - transfer learning has traditionally been solved through density ratio reweighting
% - works fine, but has the big issue of not working if the support of the source density does not cover the target density support
% - in this work, we propose instead a new paradigm for solving transfer learning
% - build a generative model for transforming samples from p1 to reproduce p0
% - adversarial learning from Goodfellow, with the practical difference that we often have only a finite set of samples (we need to avoid learning a lookup table)
% - ensure p(y|t(x)) ~ p(y|x) by having a third classifier in the mix
% - regularization
% - network architecture (propagate x, similar to highway networks)
% - experiments
% - application: fix simulated data

\bibliographystyle{apalike}
\bibliography{bibliography.bib}

\end{document}
