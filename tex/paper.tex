% Template for the submission to:
%   The Annals of Probability           [aop]
%   The Annals of Applied Probability   [aap]
%   The Annals of Statistics            [aos]
%   The Annals of Applied Statistics    [aoas]
%   Stochastic Systems                  [ssy]
%
%Author: In this template, the places where you need to add information
%        (or delete line) are indicated by {???}.  Mostly the information
%        required is obvious, but some explanations are given in lines starting
%Author:
%All other lines should be ignored.  After editing, there should be
%no instances of ??? after this line.

% use option [preprint] to remove info line at bottom
% journal options: aop,aap,aos,aoas,ssy
% natbib option: authoryear
\documentclass[aoas,preprint,authoryear]{imsart}

\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{natbib}
\RequirePackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}

\newcommand{\glnote}[1]{\textcolor{red}{[GL: #1]}}

\begin{document}

\begin{frontmatter}
%\title{Learning to Transfer across Distinct Domains with Triply Adversarial Nets}
\title{Learning to Transfer with Triply Adversarial Nets}

% indicate corresponding author with \corref{}
% \author{\fnms{John} \snm{Smith}\corref{}\ead[label=e1]{smith@foo.com}\thanksref{t1}}
% \thankstext{t1}{Thanks to somebody}
% \address{line 1\\ line 2\\ printead{e1}}
% \affiliation{Some University}

\author{\fnms{Gilles} \snm{Louppe}}
\affiliation{New York University}

\begin{abstract}

Transfer learning, and its variants known as learning under covariate shift or
domain adaptation, arises whenever test instances are governed by a distribution
that may be arbitrarily different from the distribution of the training
instances. This problem has traditionally been solved either by reweighting
training instances or by learning robust feature representations over domains.
In this work, we propose a new paradigm which consists instead in learning how
to transform training instances into test instances, possibly across input
spaces of distinct dimensions, structures or supports. For this purpose, we
extend the generative adversarial networks framework of
\cite{goodfellow2014generative} to a triply adversarial process: a transformer
network $T$ for generating test instances from training instances, a
discriminative network $D$ for estimating whether an instance comes either from
the training or the test distributions, and a classifier network $C \circ T$ for
classifying training instances in the projected space. This 3-player game
results in a network $T$ capable of transforming training instances into test
instances, while preserving the separation between classes, so that $C$ should
be an accurate classifier on test data. Experimental results demonstrate the
potential of this novative approach for transfer learning. More fundamentally,
this paradigm also raises interesting theoretical issues, since transformations
may not always be unique nor grounded, depending on the studied problem.

% other view point: C is a regularizer

\end{abstract}

\end{frontmatter}

\section{Introduction}

% - transfer learning has traditionally been solved through density ratio reweighting
% - works fine, but has the big issue of not working if the support of the source density does not cover the target density support\
%   support is needed for reweighting, but not here (Gretton et al) we can even have different space

% - in this work, we propose instead a new paradigm for solving transfer learning
% - build a generative model for transforming samples from p1 to reproduce p0
% - adversarial learning from Goodfellow, with the practical difference that we often have only a finite set of samples (we need to avoid learning a lookup table)
% - ensure p(y|t(x)) ~ p(y|x) by having a third classifier in the mix

% - application: fix simulated data

% Ganin: domains share the same input space, we dont necessarily have to
%        seek a common robust representation, here we seek instead to transform training into test

\section{Method}

% framework
% doubly adversarial network
% triply adversarial
% proofs?
% regularization

% we should state that it makes sense only when the same object (\in Omega) can exists in both space, or when there is a suitable transformation from one to the other


\section{Experiments}

% - regularization
% - network architecture (propagate x, similar to highway networks)

\section{Related work}

% also related to multi-view learning

\section{Conclusions}

\bibliographystyle{apalike}
\bibliography{bibliography.bib}

\end{document}
