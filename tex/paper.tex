\documentclass{article}

\usepackage{nips15submit_e,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}

\newcommand{\glnote}[1]{\textcolor{red}{[GL: #1]}}

\title{Learning to Transfer with Triply Adversarial Nets}


\author{
Gilles Louppe \\
New York University\\
\texttt{g.louppe@nyu.edu} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}

\maketitle

\begin{abstract}

In classification, transfer learning (or its variants known as covariate shift
or domain adaptation) arises whenever target instances are governed by a
distribution that may be arbitrarily different from the distribution of the
source instances. This problem has traditionally been solved by reweighting
approaches or by learning robust representations over domains. In this work, we
propose a new paradigm based on the assumption that the covariate shift is only
due to a different representation of the same underlying objects.
Accordingly, we propose to learn how to transform source instances into target
instances, possibly across input spaces of distinct dimensions, structures or
supports. For this purpose, we extend the generative adversarial networks
framework of \cite{goodfellow2014generative} to a triply adversarial process: a
transformer network $T$ for generating target instances from source instances, a
discriminative network $D$ for separating transformed source instances from
actual target instances, and a classifier network $C \circ T$ for classifying source
instances in the projected space. This 3-player game results in a network $T$
capable of transforming source into target instances, while preserving
separation between classes as enabled by $C$ in the adversarial setup.
Experiments demonstrate the potential of this novative approach, with promising
results when the construction of $C$ can be bootstrapped in a semi-supervised
way  from a few labeled instances from the target space.

\end{abstract}

\section{Introduction}

% - transfer learning has traditionally been solved through density ratio reweighting
% - works fine, but has the big issue of not working if the support of the source density does not cover the target density support\
%   support is needed for reweighting, but not here (Gretton et al) we can even have different space

% - in this work, we propose instead a new paradigm for solving transfer learning
% - build a generative model for transforming samples from p1 to reproduce p0
% - adversarial learning from Goodfellow, with the practical difference that we often have only a finite set of samples (we need to avoid learning a lookup table)

% other view point: start from dual adversaries, then add to the mix C is a regularizer
% - ensure p(y|t(x)) ~ p(y|x) by having a third classifier in the mix
% issue: T is not unique, how make sure samples are mapped correctly?

% - application: fix simulated data

% Ganin: domains share the same input space, we dont necessarily have to
%        seek a common robust representation, here we seek instead to transform training into test

\section{Problem statement}

Let assume a probability space $(\Omega, {\cal F}, P)$, where $\Omega$ is a
sample space, ${\cal F}$ is a set of events and $P$ is a probability measure.
Let consider a (multivariate) random variable $X: \Omega \mapsto
\mathbb{R}^p$ inducing the source distribution, along with a
finite set $\{x_i\}_{i=1}^N$ of its realizations $X(\omega_i)$, for $\omega_i
\in \Omega$.
In classification,
let us further assume that realizations from the source distribution are extended with realizations $\{y_i\}_{i=1}^N$
of a label random variable $Y:\Omega \mapsto {\cal Y}$.
Similarly, let assume a (multivariate) random variable $U: \Omega
\mapsto \mathbb{R}^q$ inducing the target distribution, along with a finite set
$\{u_j\}_{j=1}^M$ of its realizations $U(\omega_j)$, for $\omega_j \in \Omega$.
Assuming it exists, our goal is to find a transfer function $T: \mathbb{R}^p \times {\cal Y}
\mapsto \mathbb{R}^q$ such that
\begin{equation}\label{eqn:tf}
T(X(\omega), Y(\omega)) = U(\omega) ~\text{for all}~ \omega \in \Omega.
\end{equation}

Since we do not have training tuples $((X(\omega), Y(\omega)), U(\omega))$ (for the same
unknown $\omega$) from which $T$ could be learned using standard regression
algorithms, we propose instead to solve the closely related problem of
finding a transfer function $\hat T$ such that
\begin{equation}\label{eqn:tf-proxy}
P(\{ \omega | \hat T(X(\omega), Y(\omega)) = u \}) = P(\{ \omega' | U(\omega') = u \}) ~\text{for all}~ u \in \mathbb{R}^q.
\end{equation}
In words, we are looking for a transfer function $\hat T$ such that realizations
of $\hat T(X,Y)$ are undistinguisable from realizations of $U$. A function $T$ for
which Eqn.~\ref{eqn:tf} is true necessarily satisfies Eqn.~\ref{eqn:tf-proxy}.
The converse is however in general not true, since the sets of events $\{ \omega |
\hat T(X(\omega)) = u \}$ and $\{ \omega' | U(\omega') = u \}$ do not need to be
the same for the equality to hold. Accordingly, the contribution of this work is
to propose a procedure for constructing transfer functions satisfying
Eqn.~\ref{eqn:tf-proxy}, and for which Eqn.~\ref{eqn:tf} is {\it plausibly}
satisfied.




\section{Method}



% framework
% doubly adversarial network
% T is not unique => triply adversarial + regularization
% proofs?

% we should state that it makes sense only when the same object (\in Omega) can exists in both space, or when there is a suitable transformation from one to the other
% semi-supervised transfer learning: use seeds from the target space to bootstrap C

\section{Experiments}

% - regularization
% - network architecture (propagate x, similar to highway networks)

\section{Related work}

% also related to multi-view learning

\section{Conclusions}


\subsection*{Acknowledgments}

\glnote{todo}

\bibliographystyle{plain}
\bibliography{bibliography.bib}

\end{document}
